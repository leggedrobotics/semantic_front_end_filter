{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a28948",
   "metadata": {},
   "source": [
    "# Draw the concept graph\n",
    "Open rviz and use the rviz config `bagplay_concept.rviz`. You can launch ros core and rviz from the `robotparam_and_rviz.launch`\n",
    "\n",
    "The code loads a recorded rosbag, and pass the topics into registered callback functions. You can modify the following callbacks to alter the style of the visulization. What's more, you can choose a good period for the replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy # for `Duration`\n",
    "import rosbag\n",
    "from semantic_front_end_filter.common import RosbagPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules for the model\n",
    "from semantic_front_end_filter.adabins.pointcloudUtils import RaycastCamera\n",
    "from semantic_front_end_filter.adabins.models import UnetAdaptiveBins\n",
    "from semantic_front_end_filter.adabins.model_io import load_checkpoint, load_param_from_path\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from semantic_front_end_filter.Labelling.messages.imageMessage import Camera, getImageId, rgb_msg_to_image\n",
    "from semantic_front_end_filter.Labelling.messages.pointcloudMessage import rospcmsg_to_pcarray, ros_pc_msg2\n",
    "from semantic_front_end_filter.Labelling.messages.messageToVectors import msg_to_body_ang_vel, msg_to_body_lin_vel, msg_to_rotmat, msg_to_command, \\\n",
    "    msg_to_pose, msg_to_joint_positions, msg_to_joint_velocities, msg_to_joint_torques, msg_to_grav_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "rosbagpath = \"/Data/Italy_0820/Reconstruct_2022-07-18-20-34-01_0.bag\"\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-21-01-10_0.bag\" # highgrass back\n",
    "player = RosbagPlayer(rosbagpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e62d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('draw_elev_map', anonymous=False)\n",
    "player.add_publisher_of_topic(\"/alphasense_driver_ros/imu\")\n",
    "player.add_publisher_of_topic(\"/tf\", queue_size=1000)\n",
    "player.add_publisher_of_topic(\"/tf_static\")\n",
    "player.add_publisher_of_topic(\"/clock\")\n",
    "player.add_publisher_of_topic(\"/twist_mux/twist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fa680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player._callbacks = {k:v for k,v in player._callbacks.items() if k in [\"/tf\", \"/tf_static\",\"/clock\"]}\n",
    "player.play(end_time = player.bag.get_start_time()+20)\n",
    "# player.play(start_time= player.bag.get_start_time()+90,\n",
    "#             end_time = player.bag.get_start_time()+95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()# this is useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = \"2022-09-03-09-24-05_final\"\n",
    "modelname = \"2022-09-04-19-38-09_bn\"\n",
    "model_path = f\"../checkpoints/{modelname}/UnetAdaptiveBins_best.pt\"\n",
    "image_topic = \"/alphasense_driver_ros/cam4/debayered\"\n",
    "pc_topic = \"/bpearl_rear/point_cloud\"\n",
    "TF_BASE = \"base\"\n",
    "TF_MAP = \"map\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "raycastCamera = RaycastCamera(device=device) # WARN: This raycastcamera is hard coded with `tf_base_to_sensor`, however, it seems to be constant\n",
    "\n",
    "\n",
    "## Initialize model\n",
    "model_cfg = load_param_from_path(model_path)\n",
    "model_cfg[\"input_channel\"] = 4\n",
    "model = UnetAdaptiveBins.build(**model_cfg)\n",
    "\n",
    "model = load_checkpoint(model_path, model)[0]\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e20d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensor_msgs import point_cloud2\n",
    "from sensor_msgs.msg import PointCloud2, PointField\n",
    "from std_msgs.msg import Header\n",
    "from sensor_msgs.msg import Image\n",
    "from visualization_msgs.msg import Marker\n",
    "from geometry_msgs.msg import Point\n",
    "import struct\n",
    "from cv_bridge import CvBridge\n",
    "image_cv_bridge = CvBridge()\n",
    "from tf.transformations import euler_from_quaternion\n",
    "\n",
    "\n",
    "def buildPoint(x, y, z, r, g, b, a=None):\n",
    "    if(np.array([r, g, b]).max() < 1.01):\n",
    "        r = int(r * 255.0)\n",
    "        g = int(g * 255.0)\n",
    "        b = int(b * 255.0)\n",
    "        a = 255 if a is None else int(a * 255.0)\n",
    "    else:\n",
    "        r = int(r)\n",
    "        g = int(g)\n",
    "        b = int(b)\n",
    "        a = 255 if a is None else int(a)\n",
    "    rgb = struct.unpack('I', struct.pack('BBBB', b, g, r, a))[0]\n",
    "    return [x, y, z, rgb]\n",
    "rospy.init_node('draw_elev_map', anonymous=False)\n",
    "pred_pc_pub1 = rospy.Publisher(\"pc/pred_pc/pub1\", PointCloud2, queue_size=1)\n",
    "raw_pc_pub1 = rospy.Publisher(\"pc/raw_pc/pub1\", PointCloud2, queue_size=1)\n",
    "pred_pc_pub2 = rospy.Publisher(\"pc/pred_pc/pub2\", PointCloud2, queue_size=1)\n",
    "raw_pc_pub2 = rospy.Publisher(\"pc/raw_pc/pub2\", PointCloud2, queue_size=1)\n",
    "image_pub = rospy.Publisher(\"cam_image\", Image, queue_size=1)\n",
    "marker_pub = rospy.Publisher(\"pred_raw_connection\", Marker, queue_size=1)\n",
    "\n",
    "## Configs of the line_list style\n",
    "line_list_msg = Marker()\n",
    "line_list_msg.header.frame_id = \"map\"\n",
    "line_list_msg.ns = \"raw2pred\"\n",
    "line_list_msg.action = 0\n",
    "line_list_msg.type = 5\n",
    "line_list_msg.color.a = 0.2\n",
    "line_list_msg.color.r = 0.5\n",
    "line_list_msg.color.b = 0.2\n",
    "line_list_msg.color.g = 0.2\n",
    "line_list_msg.scale.x = 0.005\n",
    "line_list_msg.points = []\n",
    "\n",
    "\n",
    "player._shared_var.update(dict(\n",
    "    pcbuffer=[],\n",
    "    modeliobuffer = {\"pose\":None, \"cam_pose\":None,\n",
    "                     \"model_in\": None, \"model_out\": None} \n",
    "))\n",
    "\n",
    "def pred_and_checkerr(image, pc, pose,v):\n",
    "    \"\"\"\n",
    "    Make the prediction based on image, pointcloud and robot current pose\n",
    "    arg pose: x,y,z,rx,ry,rz,rw\n",
    "    \"\"\"\n",
    "    image = torch.Tensor(image).to(device)\n",
    "    points = torch.Tensor(pc).to(device)\n",
    "\n",
    "    # highlight the near high grass\n",
    "#     highlight_mask = (points[:,2]>pose[2]-0.2)\\\n",
    "#             & (torch.sum((points[:,:2]-torch.tensor(pose[:2]).to(device))**2, axis = 1)<2**2)\n",
    "    \n",
    "    # highlight close points\n",
    "    highlight_mask = (torch.sum((points[:,:2]-torch.tensor(pose[:2]).to(device))**2, axis = 1)<10**2)\n",
    "\n",
    "    points_highlight = points[highlight_mask] \n",
    "    \n",
    "    # get pc image\n",
    "    pc_img = torch.zeros_like(image[:1, ...],dtype = torch.float32).to(device)\n",
    "    pc_img,visible,proj_point = raycastCamera.project_cloud_to_depth(\n",
    "                    pose, points, pc_img, return_detail=True)\n",
    "    pc_img_highlight = torch.zeros_like(image[:1, ...],dtype = torch.float32).to(device)\n",
    "    pc_img_highlight = raycastCamera.project_cloud_to_depth(\n",
    "                    pose, points_highlight, pc_img_highlight)\n",
    "    # filter the points to the only visble ones\n",
    "    points = points[visible] # TODO: Decide whether should be filter the raw points\n",
    "    # get prediction\n",
    "    model_in = torch.cat([image/255., pc_img.clone()],axis=0)\n",
    "    model_in = model_in[None, ...]\n",
    "    for i, (m, s) in enumerate(zip([0.387, 0.394, 0.404, 0.120], [0.322, 0.32, 0.30,  1.17])):\n",
    "        model_in[0, i, ...] = (model_in[0, i, ...] - m)/s\n",
    "    model_out = model(model_in)\n",
    "    pred = model_out[0][0]\n",
    "    \n",
    "    v[\"modeliobuffer\"].update({\n",
    "        \"pose\": pose,\n",
    "        \"model_in\": model_in.cpu(),\n",
    "        \"model_out\": model_out.detach().cpu()\n",
    "    })\n",
    "\n",
    "    pred [(pc_img[0]==0)] = np.nan\n",
    "    pred = pred.T\n",
    "\n",
    "    # get elevation from prediction\n",
    "    highlight_mask = abs(pc_img_highlight.T.reshape(-1))>1e-9\n",
    "    \n",
    "    pred_pts = raycastCamera.project_depth_to_cloud(pose, pred)\n",
    "    pred_pts_highlight = pred_pts[highlight_mask]\n",
    "    pred_pts = pred_pts[~torch.isnan(pred_pts[:,0])]\n",
    "    # pred_pts = pred_pts[pred_pts[:,2]<pose[2]] # TODO: Decide whether this hieight mask is necessary\n",
    "    pred_points = pred_pts.detach().cpu().numpy()\n",
    "    pred_highlight_nanmask = torch.isnan(pred_pts_highlight[:,0])\n",
    "    pred_points_highlight = pred_pts_highlight[~pred_highlight_nanmask].detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "## Deprectaed way of getting raw points and colors\n",
    "#     raw_points = points.cpu().numpy().astype(pred_points.dtype) # float_64\n",
    "#     proj_point = proj_point.cpu().numpy()\n",
    "#     colors_raw = np.moveaxis((image.cpu().numpy())[:3,proj_point[:,1], proj_point[:,0]], 0, 1)\n",
    "\n",
    "    # Use reprojected raw_pts, otherwise the position of raw pc and pred pc are not aligned\n",
    "    pc_img[abs(pc_img)<1e-9] = np.nan\n",
    "    raw_pts = raycastCamera.project_depth_to_cloud(pose, pc_img.T)\n",
    "    raw_pts_highlight = raw_pts[highlight_mask]\n",
    "    raw_rgb = torch.vstack([(image[cnl].T).reshape(-1) for cnl in range(3)]).T\n",
    "    raw_rgb = (raw_rgb-raw_rgb.min())/(raw_rgb.max() - raw_rgb.min())\n",
    "    raw_rgb_highlight = raw_rgb[highlight_mask]\n",
    "    raw_rgb = raw_rgb[~torch.isnan(raw_pts[:,0])]\n",
    "    raw_pts = raw_pts[~torch.isnan(raw_pts[:,0])]\n",
    "    raw_points = raw_pts.cpu().numpy()\n",
    "    raw_colors = raw_rgb.cpu().numpy()\n",
    "    \n",
    "    raw_highlight_nanmask = torch.isnan(raw_pts_highlight[:,0])\n",
    "    raw_points_highlight = raw_pts_highlight[~raw_highlight_nanmask].cpu().numpy()\n",
    "    raw_colors_highlight = raw_rgb_highlight[~raw_highlight_nanmask].cpu().numpy()\n",
    "    \n",
    "    \n",
    "    header = Header()\n",
    "    header.frame_id = \"map\"\n",
    "    fields = [\n",
    "        PointField('x', 0, PointField.FLOAT32, 1),\n",
    "        PointField('y', 4, PointField.FLOAT32, 1),\n",
    "        PointField('z', 8, PointField.FLOAT32, 1),\n",
    "        # PointField('rgb', 12, PointField.UINT32, 1),\n",
    "        PointField('rgba', 12, PointField.UINT32, 1),\n",
    "    ]\n",
    "    \n",
    "    color_pred_high = np.array([[255,170,0.]])/256\n",
    "    color_pred_low = np.array([[80,60,0.]])/256\n",
    "    colors_pred = np.clip(1.5*(pred_points[:, 2]-pred_points[:, 2].min())\n",
    "        /(pred_points[:, 2].max()-pred_points[:, 2].min()),0.,1.)[:,None] \\\n",
    "        * (color_pred_high- color_pred_low) + color_pred_low\n",
    "    cloud_pred = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], *c) for p, c in zip(pred_points, colors_pred)])\n",
    "    cloud_pred_highlight = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], 0.,0.,0.) for p in pred_points_highlight])\n",
    "    pred_pc_pub1.publish(cloud_pred)\n",
    "    pred_pc_pub2.publish(cloud_pred_highlight)\n",
    "    \n",
    "\n",
    "    \n",
    "    cloud_raw = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], *c) for p, c in zip(raw_points, raw_colors)])\n",
    "    cloud_raw_highlight = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], *c) for p, c in zip(raw_points_highlight, raw_colors_highlight)])\n",
    "    \n",
    "    raw_pc_pub1.publish(cloud_raw)\n",
    "    raw_pc_pub2.publish(cloud_raw_highlight)\n",
    "    \n",
    "    line_list_msg.points = [ p\n",
    "        for pp, rp in zip(pred_points_highlight[::300], raw_points_highlight[::300])\n",
    "#         for p in [Point(*pp[:3]), Point(*rp[:3])]\n",
    "        for p in [Point(*raycastCamera.camera.pose[0]), Point(*rp[:3])]\n",
    "    ]\n",
    "#     line_list_msg.points = [ p\n",
    "#         for pp, rp in zip(pred_points[::300], raw_points[::300])\n",
    "# #         for p in [Point(*pp[:3]), Point(*rp[:3])]\n",
    "#         for p in [Point(*raycastCamera.camera.pose[0]), Point(*rp[:3])]\n",
    "#     ]\n",
    "    marker_pub.publish(line_list_msg)\n",
    "    \n",
    "    image_to_show = np.moveaxis(image.cpu().numpy(), 0, 2).astype(np.uint8)\n",
    "    image_pub.publish(image_cv_bridge.cv2_to_imgmsg(image_to_show, \"bgr8\"))\n",
    "\n",
    "\n",
    "def image_cb(topic, msg, t, tf_buffer, v):\n",
    "    if(not len(v[\"pcbuffer\"])): return\n",
    "\n",
    "    img = rgb_msg_to_image(msg, raycastCamera.camera.is_debayered, raycastCamera.camera.rb_swap, (\"compressed\" in topic))\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "\n",
    "    if not (tf_buffer.can_transform_core(TF_MAP, TF_BASE,  msg.header.stamp)[0]): return \n",
    "    tf = tf_buffer.lookup_transform_core(TF_MAP, TF_BASE, msg.header.stamp)\n",
    "    pose = msg_to_pose(tf)  # pose in fixed ref frame (odom or map)\n",
    "\n",
    "    pc = np.concatenate(v[\"pcbuffer\"],axis = 0)\n",
    "    pred_and_checkerr(img, pc, pose, v)\n",
    "    cam_tf = tf_buffer.lookup_transform_core(TF_MAP, \"cam4_sensor_frame\",  msg.header.stamp)\n",
    "    cam_pose = msg_to_pose(cam_tf)\n",
    "    v[\"modeliobuffer\"].update({\n",
    "        \"cam_pose\": cam_pose\n",
    "    })\n",
    "\n",
    "#     v[\"pcbuffer\"] = v[\"pcbuffer\"][-10:]\n",
    "    v[\"pcbuffer\"] = []\n",
    "\n",
    "player.register_callback(image_topic, image_cb)\n",
    "\n",
    "\n",
    "def pointcloud_cb(topic, msg, t, tf_buffer, v):\n",
    "    if not (tf_buffer.can_transform_core(TF_MAP, msg.header.frame_id,  msg.header.stamp)[0]): return\n",
    "    tf = tf_buffer.lookup_transform_core(TF_MAP, msg.header.frame_id,  msg.header.stamp)\n",
    "    pose = msg_to_pose(tf)\n",
    "    pc_array = rospcmsg_to_pcarray(msg, pose)\n",
    "\n",
    "#     v[\"pcbuffer\"].append(pc_array[:,:3])\n",
    "    v[\"pcbuffer\"] = [pc_array[:,:3]]\n",
    "    \n",
    "player.register_callback(pc_topic, pointcloud_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b51bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player._callbacks.pop(\"/bpearl_rear/point_cloud\")\n",
    "player._callbacks.pop(\"/alphasense_driver_ros/cam4/debayered\")\n",
    "player._callbacks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# player.play(start_time= player.bag.get_start_time()+450, # the time for concept figure\n",
    "#             end_time = player.bag.get_start_time()+452)\n",
    "# player.play(start_time= player.bag.get_start_time()+422, # the time for system figure\n",
    "#             end_time = player.bag.get_start_time()+424)\n",
    "player.play(start_time= player.bag.get_start_time()+450, \n",
    "            end_time = player.bag.get_start_time()+457)\n",
    "# player.play(end_time = player.bag.get_start_time()+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b37713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"../tmp/systemmodel.pkl\",\"wb\") as f:\n",
    "    pkl.dump(player._shared_var[\"modeliobuffer\"],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54061686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def maxdilation(img):\n",
    "    out = np.ones_like(img) * img.min()\n",
    "    out[:-1,:-1] = np.max(np.stack([out[:-1,:-1], img[1:,1:]]), axis = 0)\n",
    "    out[:-1,:] = np.max(np.stack([out[:-1,:], img[1:,:]]), axis = 0)\n",
    "    out[:-1,1:] = np.max(np.stack([out[:-1,1:], img[1:,:-1]]), axis = 0)\n",
    "    out[:,:-1] = np.max(np.stack([out[:,:-1], img[:,1:]]), axis = 0)\n",
    "    out[:,:] = img[:,:]\n",
    "    out[:,1:] = np.max(np.stack([out[:,1:], img[:,:-1]]), axis = 0)\n",
    "    out[1:,:-1] = np.max(np.stack([out[1:,:-1], img[:-1,1:]]), axis = 0)\n",
    "    out[1:,:] = np.max(np.stack([out[1:,:], img[:-1,:]]), axis = 0)\n",
    "    out[1:,1:] = np.max(np.stack([out[1:,1:], img[:-1,:-1]]), axis = 0)\n",
    "    return out\n",
    "\n",
    "pred = player._shared_var[\"modeliobuffer\"][\"model_out\"][0,0].numpy()\n",
    "pc_img = player._shared_var[\"modeliobuffer\"][\"model_in\"][0,3].numpy()\n",
    "pred [(pc_img<1e-9)] = 0\n",
    "\n",
    "pred_dilation = maxdilation(pred)\n",
    "pred_dilation[pred_dilation<1e-9] = np.nan\n",
    "\n",
    "pcimg_dilation = maxdilation(pc_img)\n",
    "pcimg_dilation[pcimg_dilation<1e-9] = np.nan\n",
    "\n",
    "# plt.imshow(pred_dilation)\n",
    "plt.figure(figsize=(12,16))\n",
    "print(\"pred_dilation\", pred_dilation[~np.isnan(pred_dilation)].min(), pred_dilation[~np.isnan(pred_dilation)].max())\n",
    "print(\"pcimg_dilation\", pcimg_dilation[~np.isnan(pcimg_dilation)].min(), pcimg_dilation[~np.isnan(pcimg_dilation)].max())\n",
    "plt.imshow(pcimg_dilation, vmin = 0, vmax = 15, cmap = \"plasma\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/home/chenyu/Pictures/system/system_pcimg.png\", bbox_inches='tight', pad_inches=0)\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.imshow(pred_dilation, vmin = 0, vmax = 10, cmap = \"plasma\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/home/chenyu/Pictures/system/system_predimg.png\", bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_front_end_filter.Labelling.ExtractDepthImage import DIFG\n",
    "from ruamel.yaml import YAML\n",
    "cfg = YAML().load(open(\"../semantic_front_end_filter/Labelling/data_extraction_SA.yaml\", 'r'))\n",
    "GroundMap_filepath = \"/Data/Italy_0820/GroundMap.msgpack\"\n",
    "depth_img_cam = DIFG(GroundMap_filepath, cfg['calibration'], 'cam4', cfg)\n",
    "depth_img_cam.DATASET_TYPE = \"Italy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pose = player._shared_var[\"modeliobuffer\"][\"cam_pose\"]\n",
    "position = np.array(pose[:3])\n",
    "euler = np.array(euler_from_quaternion(pose[3:]))\n",
    "d_img, v_img = depth_img_cam.getDImage(transition=position, rotation=euler, ratation_is_matrix=False)\n",
    "d_img[v_img>0.03] = np.nan\n",
    "d_img[d_img > 20] = np.nan\n",
    "fig = plt.figure(figsize=(12,16))\n",
    "plt.imshow(d_img, vmin = 0, vmax = 10, cmap = \"plasma\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"/home/chenyu/Pictures/system/system_trajlabel.png\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29479a5",
   "metadata": {},
   "source": [
    "# Draw the foot holds\n",
    "\n",
    "use the rviz config file `semantic_front_end_filter_ros/configs/bagplay_footholds.rviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy \n",
    "import numpy as np\n",
    "from semantic_front_end_filter.Labelling.GroundfromTrajs import GFT\n",
    "from visualization_msgs.msg import Marker, MarkerArray\n",
    "from geometry_msgs.msg import Pose, Quaternion, Point\n",
    "from std_msgs.msg import Header, Float32MultiArray, MultiArrayLayout, MultiArrayDimension\n",
    "\n",
    "\n",
    "foottrajpath = \"/Data/Italy_0820/FeetTrajs.msgpack\"\n",
    "\n",
    "gft = GFT(FeetTrajsFile = foottrajpath, InitializeGP=False)\n",
    "foot_holds = {k : np.array(gft.getContactPoints(v)[0]) for k,v in gft.FeetTrajs.items()} # A dict of the contact points of each foot\n",
    "# foot_holds_array = np.vstack([v[-4000:-1200] for v in foot_holds.values()])\n",
    "foot_holds_array = np.vstack(list(foot_holds.values()))\n",
    "# rosbagpath = \"/Data/Italy_0820/Reconstruct_2022-07-18-20-34-01_0.bag\"\n",
    "# rospy.init_node('draw_elev_map', anonymous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "footholds_pub = rospy.Publisher(\"labeling/footholds\", Marker, queue_size=1)\n",
    "\n",
    "\n",
    "line_list_msg = Marker()\n",
    "line_list_msg.header.frame_id = \"map\"\n",
    "line_list_msg.ns = \"footholds\"\n",
    "line_list_msg.action = 0\n",
    "line_list_msg.type = Marker.SPHERE_LIST\n",
    "line_list_msg.color.a = 1\n",
    "line_list_msg.color.r = 67./256\n",
    "line_list_msg.color.g = 110./256\n",
    "line_list_msg.color.b = 176./256\n",
    "line_list_msg.scale.x = 0.04\n",
    "line_list_msg.points = []\n",
    "\n",
    "line_list_msg.points = [Point(*p[:3]) for p in foot_holds_array]\n",
    "\n",
    "footholds_pub.publish(line_list_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271aa4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_pub = rospy.Publisher(\"labeling/convenientaxis\", MarkerArray, queue_size=1)\n",
    "marr = MarkerArray()\n",
    "marr.markers = [\n",
    "    Marker(\n",
    "        header = Header(frame_id = \"map\"),\n",
    "        ns = \"axis\",\n",
    "        color = type(Marker().color)(\n",
    "            0,0,0,1\n",
    "        ),\n",
    "        scale = type(Marker().scale)(\n",
    "            8, 0.1, 0.1\n",
    "        ),\n",
    "        pose = Pose(\n",
    "            position = Point(-16, 38, -8)\n",
    "        ),\n",
    "        id = i\n",
    "        \n",
    "    )\n",
    "    for i in range(3)\n",
    "]\n",
    "marr.markers[0].type = Marker.ARROW\n",
    "marr.markers[1].type = Marker.ARROW\n",
    "marr.markers[2].type = Marker.ARROW\n",
    "\n",
    "\n",
    "marr.markers[0].pose.orientation = Quaternion(0,0,0,1)\n",
    "marr.markers[1].pose.orientation = Quaternion(0,0,1/np.sqrt(2), -1/np.sqrt(2))\n",
    "marr.markers[2].pose.orientation = Quaternion(0,-1/np.sqrt(2), 0, 1/np.sqrt(2))\n",
    "axis_pub.publish(marr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98ed88",
   "metadata": {},
   "source": [
    "### Draw the reconstructed surface with gridmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d559ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_map_msgs.msg import GridMap, GridMapInfo\n",
    "import msgpack\n",
    "import msgpack_numpy as m\n",
    "m.patch()\n",
    "\n",
    "# groundmappath = \"/Data/Italy_0820/GroundMap.msgpack\"\n",
    "groundmappath = \"/Data/tmp/tmp1/GroundMap.msgpack\"\n",
    "with open(groundmappath, \"rb\") as data_file:\n",
    "    data = data_file.read()\n",
    "    ground_dict = msgpack.unpackb(data)\n",
    "    print(\"load ground dict, y real range: \",ground_dict[\"yRealRange\"], \n",
    "                            \"x real range: \", ground_dict[\"xRealRange\"])\n",
    "    ground_dict = ground_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gp_surface_pub = rospy.Publisher(\"labeling/grid_map\", GridMap, queue_size=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmap = np.array(ground_dict[\"GPMap\"])\n",
    "gpmap[np.array(ground_dict[\"Confidence\"])>0.03] = np.nan\n",
    "gpmap = gpmap[::-1, ::-1]\n",
    "gpmap = gpmap.T\n",
    "\n",
    "gpmapmsg = GridMap(\n",
    "    info = GridMapInfo(\n",
    "        header= Header(frame_id=\"map\"),\n",
    "        resolution = ground_dict[\"res\"],\n",
    "        length_x = ground_dict[\"res\"] * gpmap.shape[1],\n",
    "        length_y = ground_dict[\"res\"] * gpmap.shape[0],\n",
    "        pose = Pose(\n",
    "#             position = Point(\n",
    "#                 ground_dict[\"xRealRange\"][1] - ground_dict[\"res\"] * gpmap.shape[0]/2,\n",
    "#                 ground_dict[\"yRealRange\"][1] - ground_dict[\"res\"] * gpmap.shape[1]/2,0),\n",
    "            position = Point(\n",
    "                (gpmap.shape[1]/2+ground_dict[\"xNormal\"])*ground_dict[\"res\"],\n",
    "                (gpmap.shape[0]/2+ground_dict[\"yNormal\"])*ground_dict[\"res\"],0),\n",
    "            orientation = Quaternion(x=0,y=0,z=0,w=1)\n",
    "        )),\n",
    "    layers = [\"elevation\"],\n",
    "    basic_layers = [\"elevation\"],\n",
    "    data = [\n",
    "        Float32MultiArray(\n",
    "            layout = MultiArrayLayout(\n",
    "                dim = [\n",
    "                    MultiArrayDimension(\n",
    "                        label= [\"column_index\", \"row_index\"][i],\n",
    "                        size = d,\n",
    "                        stride = np.prod( gpmap.shape[i:])\n",
    "                    )for i,d in enumerate(gpmap.shape)\n",
    "                ]\n",
    "            ),\n",
    "            data = gpmap.reshape(-1).tolist()\n",
    "            \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "gpmapmsg.info.header.stamp = rospy.Time(rospy.Time.now().to_sec()+0.04)\n",
    "gp_surface_pub.publish(gpmapmsg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_gp_surface(topic, msg, t, tf_buffer, v):\n",
    "    msg = gpmapmsg\n",
    "    msg.info.header.stamp = rospy.Time(rospy.Time.now().to_sec()+0.001)\n",
    "    gp_surface_pub.publish(gpmapmsg)\n",
    "player.register_callback(pc_topic, pub_gp_surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06caa800",
   "metadata": {},
   "source": [
    "#### Color conversion in gridmap\n",
    "\n",
    "grid_map use a float number to represent the rgb color of the map. The conversion [code](http://docs.ros.org/en/jade/api/grid_map_core/html/GridMapMath_8cpp_source.html) defns the function\n",
    "\n",
    "```c++\n",
    "bool colorVectorToValue(const Eigen::Vector3i& colorVector, unsigned long& colorValue)\n",
    "{\n",
    "  colorValue = ((int)colorVector(0)) << 16 | ((int)colorVector(1)) << 8 | ((int)colorVector(2));\n",
    "  return true;\n",
    "}\n",
    "```\n",
    "\n",
    "The way to define color should be \n",
    "```python\n",
    "color = np.array([10*2**16 + 200*2**8 + 123],dtype = np.uint32)\n",
    "color.view(np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70c367",
   "metadata": {},
   "source": [
    "## Annotate position elevation map\n",
    "Draw the scale of the elevation map, the position of the camera, and the field of view of the robot\n",
    "\n",
    "use the rviz config file `semantic_front_end_filter_ros/configs/bagplay_eleverr.rviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "import math\n",
    "import tf\n",
    "import numpy as np\n",
    "import geometry_msgs.msg\n",
    "from visualization_msgs.msg import Marker, MarkerArray\n",
    "from geometry_msgs.msg import Pose, Quaternion\n",
    "from std_msgs.msg import Header, Float32MultiArray, MultiArrayLayout, MultiArrayDimension\n",
    "from geometry_msgs.msg import Point\n",
    "\n",
    "br = tf.TransformBroadcaster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put the robot model to origin\n",
    "br.sendTransform((0, 0, 0),\n",
    "    tf.transformations.quaternion_from_euler(0, 0, 0),\n",
    "    rospy.Time(rospy.Time.now().to_sec()+0.001),\n",
    "    \"base\",\n",
    "     \"map\")\n",
    "\n",
    "def get_frames_from_tf_buffer(tf_buffer):\n",
    "    def extract_frame_and_parent(frameline):\n",
    "        \"\"\"\n",
    "        The frameline is something like:\n",
    "        Frame camera_init exists with parent camera_init_CORRECTED.\n",
    "        \"\"\"\n",
    "        frameline = frameline[:-1].split()\n",
    "        return frameline[1], frameline[-1]\n",
    "\n",
    "    frame_maps = [\n",
    "        extract_frame_and_parent(line)\n",
    "        for line in \n",
    "            tf_buffer.all_frames_as_string().split('\\n')\n",
    "        if line[:5] == \"Frame\"\n",
    "    ]\n",
    "    frame_parent_maps = {f:p for f,p in frame_maps}\n",
    "    # frame_parent_maps\n",
    "    frame_child_maps = {}\n",
    "    for f,p in frame_parent_maps.items():\n",
    "        s = frame_child_maps.get(p,set())\n",
    "        s.add(f)\n",
    "        frame_child_maps[p] = s\n",
    "    return frame_parent_maps, frame_child_maps\n",
    "\n",
    "def child_links(base, frame_child_maps):\n",
    "    que = [base]\n",
    "    while(len(que)):\n",
    "        base = que.pop()\n",
    "        for c in frame_child_maps.get(base,[]):\n",
    "            que.append(c)\n",
    "            yield c, base\n",
    "\n",
    "\n",
    "frame_parent_maps, frame_child_maps = get_frames_from_tf_buffer(player.tf_buffer)\n",
    "for f,p in child_links(\"base\", frame_child_maps):\n",
    "    transf = player.tf_buffer.lookup_transform_core(p, f, rospy.Time(0))\n",
    "    pose = msg_to_pose(transf)  # pose in fixed ref frame (odom or map)\n",
    "    br.sendTransform(pose[:3],\n",
    "        pose[3:],\n",
    "        rospy.Time(rospy.Time.now().to_sec()+1),\n",
    "        f,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the grid map\n",
    "from grid_map_msgs.msg import GridMap, GridMapInfo\n",
    "gp_surface_pub = rospy.Publisher(\"elev/dumm_grid_map\", GridMap, queue_size=1);\n",
    "\n",
    "res = 0.04\n",
    "len_x = len_y = 8\n",
    "gpmap = np.zeros((int(len_x/res), int(len_y/res)))\n",
    "\n",
    "gpmapmsg = GridMap(\n",
    "    info = GridMapInfo(\n",
    "        header= Header(frame_id=\"map\"),\n",
    "        resolution = res,\n",
    "        length_x = len_x,\n",
    "        length_y = len_y,\n",
    "        pose = Pose(\n",
    "            position = Point(0,0,-2),\n",
    "            orientation = Quaternion(x=0,y=0,z=0,w=1)\n",
    "        )),\n",
    "    layers = [\"elevation\"],\n",
    "    basic_layers = [\"elevation\"],\n",
    "    data = [\n",
    "        Float32MultiArray(\n",
    "            layout = MultiArrayLayout(\n",
    "                dim = [\n",
    "                    MultiArrayDimension(\n",
    "                        label= [\"column_index\", \"row_index\"][i],\n",
    "                        size = d,\n",
    "                        stride = np.prod( gpmap.shape[i:])\n",
    "                    )for i,d in enumerate(gpmap.shape)\n",
    "                ]\n",
    "            ),\n",
    "            data = gpmap.reshape(-1).tolist()\n",
    "            \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "gp_surface_pub.publish(gpmapmsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the camera FOV\n",
    "from semantic_front_end_filter.adabins.pointcloudUtils import RaycastCamera\n",
    "from scipy.spatial.transform import Rotation\n",
    "raycastCamera = RaycastCamera()\n",
    "camera = raycastCamera.camera\n",
    "camera.update_pose_from_base_pose([0,0,0, 0,0,0,1])\n",
    "pose = camera.pose\n",
    "cam_fx = camera.camera_matrix[0,0]\n",
    "cam_fy = camera.camera_matrix[1,1]\n",
    "cam_h = camera.image_height\n",
    "cam_w = camera.image_width\n",
    "\n",
    "fovx = 2*np.arctan(cam_w/(2*cam_fx))\n",
    "fovy = 2*np.arctan(cam_h/(2*cam_fy))\n",
    "heading = camera.pose[1][:3, 2] * 5\n",
    "\n",
    "rot1 = Rotation.from_rotvec(fovx/2 * np.array([0, 0, 1])).as_matrix()\n",
    "rot2 = Rotation.from_rotvec(-fovx/2 * np.array([0, 0, 1])).as_matrix()\n",
    "\n",
    "head1 = rot1 @(heading-pose[0]) + pose[0]\n",
    "head2 = rot2 @(heading-pose[0]) + pose[0]\n",
    "axis_pub = rospy.Publisher(\"elev/fovaxis\", MarkerArray, queue_size=1)\n",
    "marr = MarkerArray()\n",
    "marr.markers = [\n",
    "    Marker(\n",
    "        header = Header(frame_id = \"map\"),\n",
    "        ns = \"fov\",\n",
    "        color = type(Marker().color)(\n",
    "            0,0,0,1\n",
    "        ),\n",
    "        scale = type(Marker().scale)(\n",
    "            0.05, 0.1, 0\n",
    "        ),\n",
    "        id = i,\n",
    "        points = [Point(*pose[0]) ]\n",
    "    )\n",
    "    for i in range(2)\n",
    "]\n",
    "marr.markers[0].type = Marker.ARROW\n",
    "marr.markers[1].type = Marker.ARROW\n",
    "\n",
    "marr.markers[0].points.append(Point(*head1))\n",
    "marr.markers[1].points.append(Point(*head2))\n",
    "\n",
    "axis_pub.publish(marr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_pub.publish(marr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c635aab",
   "metadata": {},
   "source": [
    "## Elevation map gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy # for `Duration`\n",
    "import rosbag\n",
    "from semantic_front_end_filter.common import RosbagPlayer\n",
    "from semantic_front_end_filter.common.gridmap_ros_util import build_gridmap_msg\n",
    "from sensor_msgs.msg import Image\n",
    "\n",
    "from geometry_msgs.msg import Pose, Quaternion, Point\n",
    "from std_msgs.msg import Header, Float32MultiArray, MultiArrayLayout, MultiArrayDimension\n",
    "\n",
    "from sensor_msgs import point_cloud2\n",
    "from sensor_msgs.msg import PointCloud2, PointField\n",
    "\n",
    "from cv_bridge import CvBridge\n",
    "image_cv_bridge = CvBridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_front_end_filter.adabins.pointcloudUtils import RaycastCamera\n",
    "from semantic_front_end_filter.adabins.models import UnetAdaptiveBins\n",
    "from semantic_front_end_filter.adabins.model_io import load_checkpoint, load_param_from_path\n",
    "from semantic_front_end_filter.adabins.elevation_vis import WorldViewElevationMap\n",
    "from semantic_front_end_filter.adabins.elevation_eval_util import ElevationMapEvaluator\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "from semantic_front_end_filter.Labelling.messages.imageMessage import Camera, getImageId, rgb_msg_to_image\n",
    "from semantic_front_end_filter.Labelling.messages.pointcloudMessage import rospcmsg_to_pcarray, ros_pc_msg2\n",
    "from semantic_front_end_filter.Labelling.messages.messageToVectors import msg_to_body_ang_vel, msg_to_body_lin_vel, msg_to_rotmat, msg_to_command, \\\n",
    "    msg_to_pose, msg_to_joint_positions, msg_to_joint_velocities, msg_to_joint_torques, msg_to_grav_vec\n",
    "\n",
    "from grid_map_msgs.msg import GridMap, GridMapInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rosbagpath = \"/Data/Italy_0820/Reconstruct_2022-07-18-20-34-01_0.bag\"\n",
    "groundmappath = \"/Data/Italy_0820/GroundMap.msgpack\"\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-08-48-50_0.bag\"\n",
    "# groundmappath = \"/Data/hongeberg/extract/Reconstruct_2022-08-13-08-48-50_0/GroundMap.msgpack\"\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-10-08-26_0.bag\"\n",
    "# groundmappath = \"/Data/hongeberg/extract/Reconstruct_2022-08-13-08-48-50_0/GroundMap.msgpack\"\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-16-48-24_0.bag\" bump into stone\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-20-39-56_0.bag\" # highgrass back\n",
    "# rosbagpath = \"/Data/hongeberg/mission_data/Reconstruct/Reconstruct_2022-08-13-21-01-10_0.bag\" # \n",
    "\n",
    "player = RosbagPlayer(rosbagpath)\n",
    "rospy.init_node('draw_elev_map', anonymous=False)\n",
    "player.add_publisher_of_topic(\"/alphasense_driver_ros/imu\")\n",
    "player.add_publisher_of_topic(\"/tf\", queue_size=1000)\n",
    "player.add_publisher_of_topic(\"/tf_static\")\n",
    "player.add_publisher_of_topic(\"/clock\")\n",
    "player.add_publisher_of_topic(\"/twist_mux/twist\")\n",
    "\n",
    "player.add_publisher_of_topic(\"/alphasense_driver_ros/cam4/debayered\")\n",
    "player.add_publisher_of_topic(\"/bpearl_rear/point_cloud\")\n",
    "player.add_publisher_of_topic(\"/bpearl_front/point_cloud\")\n",
    "\n",
    "player.play(end_time = player.bag.get_start_time()+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = \"2022-09-04-19-38-09_bn\"\n",
    "# modelname = \"\"\n",
    "modelname = \"2022-08-29-23-51-44\"\n",
    "# modelname = \"2022-09-03-09-24-05_final\"\n",
    "# modelname = \"2022-09-14-10-15-03\"\n",
    "model_path = f\"../checkpoints/{modelname}/UnetAdaptiveBins_best.pt\"\n",
    "image_topic = \"/alphasense_driver_ros/cam4/debayered\"\n",
    "pc_topic = \"/bpearl_rear/point_cloud\"\n",
    "TF_BASE = \"base\"\n",
    "TF_MAP = \"map\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "raycastCamera = RaycastCamera(device=device) # WARN: This raycastcamera is hard coded with `tf_base_to_sensor`, however, it seems to be constant\n",
    "\n",
    "elevation_pred = WorldViewElevationMap(resolution = None, map_length = None, init_with_initialize_map = None)\n",
    "elevation_pc = WorldViewElevationMap(resolution = None, map_length = None, init_with_initialize_map = None)\n",
    "\n",
    "evaluator = ElevationMapEvaluator(groundmappath, elevation_pred.param)\n",
    "\n",
    "## Initialize model\n",
    "model_cfg = load_param_from_path(model_path)\n",
    "model_cfg[\"input_channel\"] = 4\n",
    "model = UnetAdaptiveBins.build(**model_cfg)\n",
    "\n",
    "model = load_checkpoint(model_path, model)[0]\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9def17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_surface_pub = rospy.Publisher(\"elev/pred_gridmap\", GridMap, queue_size=1)\n",
    "raw_surface_pub = rospy.Publisher(\"elev/raw_gridmap\", GridMap, queue_size=1)\n",
    "pred_pc_pub = rospy.Publisher(\"pointcloud/pred_pc\", PointCloud2, queue_size=1)\n",
    "raw_pc_pub = rospy.Publisher(\"pointcloud/raw_pc\", PointCloud2, queue_size=1)\n",
    "\n",
    "image_pub = rospy.Publisher(\"cam_image\", Image, queue_size=1)\n",
    "res = elevation_pred.resolution\n",
    "len_x = len_y = res * (elevation_pred.cell_n-2)\n",
    "\n",
    "elevmsg_cache_pred = None\n",
    "elevmsg_cache_raw = None\n",
    "\n",
    "import struct\n",
    "def buildPoint(x, y, z, r, g, b, a=None):\n",
    "    if(np.array([r, g, b]).max() < 1.01):\n",
    "        r = int(r * 255.0)\n",
    "        g = int(g * 255.0)\n",
    "        b = int(b * 255.0)\n",
    "        a = 255 if a is None else int(a * 255.0)\n",
    "    else:\n",
    "        r = int(r)\n",
    "        g = int(g)\n",
    "        b = int(b)\n",
    "        a = 255 if a is None else int(a)\n",
    "    rgb = struct.unpack('I', struct.pack('BBBB', b, g, r, a))[0]\n",
    "    return [x, y, z, rgb]\n",
    "\n",
    "\n",
    "player._shared_var.update(dict(\n",
    "    pcbuffer=[],\n",
    "    height_range=(),\n",
    "    elevmsg_buffer = (),\n",
    "    pose_buffer = (),\n",
    "    elevmap_buffer = ()\n",
    "))\n",
    "def pred_and_checkerr(image, pc, pose, v):\n",
    "    \"\"\"\n",
    "    Make the prediction based on image, pointcloud and robot current pose\n",
    "    arg pose: x,y,z,rx,ry,rz,rw\n",
    "    \"\"\"\n",
    "    image = torch.Tensor(image).to(device)\n",
    "    points = torch.Tensor(pc).to(device)\n",
    "    # pose = torch.Tensor(pose).to(device)\n",
    "    # get pc image\n",
    "    pc_img = torch.zeros_like(image[:1, ...]).to(device).float()\n",
    "    pc_img,visible,proj_point = raycastCamera.project_cloud_to_depth(\n",
    "                    pose, points, pc_img, return_detail=True)\n",
    "    # filter the points to the only visble ones\n",
    "    points = points[visible] # TODO: Decide whether should be filter the raw points\n",
    "    # get prediction\n",
    "    model_in = torch.cat([image/255., pc_img],axis=0)\n",
    "    model_in = model_in[None, ...]\n",
    "    for i, (m, s) in enumerate(zip([0.387, 0.394, 0.404, 0.120], [0.322, 0.32, 0.30,  1.17])):\n",
    "        model_in[0, i, ...] = (model_in[0, i, ...] - m)/s\n",
    "    pred = model(model_in)[0][0]\n",
    "\n",
    "    pred [(pc_img[0]==0)] = np.nan\n",
    "    pred[pred<pc_img[0]] = np.nan\n",
    "    pred = pred.T\n",
    "\n",
    "    # get elevation from prediction\n",
    "    pred_pts = raycastCamera.project_depth_to_cloud(pose, pred)\n",
    "    pred_pts = pred_pts[~torch.isnan(pred_pts[:,0])]\n",
    "    pred_points = pred_pts.detach().cpu().numpy()\n",
    "    points = points.cpu().numpy().astype(pred_points.dtype) # float_64\n",
    "    pc_img[abs(pc_img)<1e-9] = np.nan\n",
    "    raw_pts = raycastCamera.project_depth_to_cloud(pose, pc_img.T)\n",
    "    raw_pts = raw_pts[~torch.isnan(raw_pts[:,0])]\n",
    "    raw_points = raw_pts.cpu().numpy()\n",
    "\n",
    "    elevation_pred.move_to_and_input(pose[0:3], pred_points)\n",
    "    elevmap_pred = elevation_pred.get_elevation_map()\n",
    "    travmap_pred = elevation_pred.get_layer_map(\"traversability\")\n",
    "#     minfmap_pred = elevation_pred.get_layer_map(\"min_filter\")\n",
    "#     smotmap_pred = elevation_pred.get_layer_map(\"smooth\")\n",
    "    \n",
    "    elevation_pc.move_to_and_input(pose[0:3], points)\n",
    "    elevmap_pc = elevation_pc.get_elevation_map()\n",
    "    travmap_pc = elevation_pc.get_layer_map(\"traversability\")\n",
    "#     minfmap_pc = elevation_pc.get_layer_map(\"min_filter\")\n",
    "#     smotmap_pc = elevation_pc.get_layer_map(\"smooth\")\n",
    "    if(not (~np.isnan(elevmap_pc)).sum()):\n",
    "        print(\"no elev map constructed\")\n",
    "    else:\n",
    "        v[\"height_range\"] = (elevmap_pc[~np.isnan(elevmap_pc)].min(), \n",
    "                             elevmap_pc[~np.isnan(elevmap_pc)].max())\n",
    "    \n",
    "    predmapmsg = build_gridmap_msg({\n",
    "        \"elevation\": elevmap_pred.T,\n",
    "        \"traversability\": travmap_pred.T,\n",
    "#         \"smooth\": smotmap_pred.T,\n",
    "    }, res, len_x, len_y, pose[0:3])\n",
    "    if elevmsg_cache_pred is None:\n",
    "        pred_surface_pub.publish(predmapmsg)\n",
    "    else:\n",
    "        pred_surface_pub.publish(elevmsg_cache_pred)\n",
    "    \n",
    "    rawmapmsg = build_gridmap_msg({\n",
    "        \"elevation\": elevmap_pc.T,\n",
    "        \"traversability\": travmap_pc.T,\n",
    "#         \"smooth\": smotmap_pc.T,\n",
    "    }, res, len_x, len_y, pose[0:3])\n",
    "    if elevmsg_cache_raw is None:\n",
    "        raw_surface_pub.publish(rawmapmsg)\n",
    "    else:\n",
    "        raw_surface_pub.publish(elevmsg_cache_raw)\n",
    "    \n",
    "#     v[\"elevmap_buffer\"] = ((elevmap_pred, smotmap_pred), \n",
    "#                            (elevmap_pc, smotmap_pc))\n",
    "#     v[\"elevmsg_buffer\"] = (predmapmsg, rawmapmsg)\n",
    "    v[\"pose_buffer\"] = pose\n",
    "    \n",
    "    image_to_show = np.moveaxis(image.cpu().numpy(), 0, 2).astype(np.uint8)\n",
    "    image_pub.publish(image_cv_bridge.cv2_to_imgmsg(image_to_show, \"bgr8\"))\n",
    "    \n",
    "    \n",
    "    # pointcloud_publish\n",
    "    ## Transform into the base frame\n",
    "    rot_base = Rotation.from_quat(pose[3:]).as_matrix()\n",
    "#     raw_points = (rot_base.T @ (raw_points - pose[:3]).T).T\n",
    "#     pred_points = (rot_base.T @ (pred_points - pose[:3]).T).T\n",
    "    raw_points = (raw_points - pose[:3]) @ rot_base \n",
    "    pred_points = (pred_points - pose[:3]) @ rot_base \n",
    "    \n",
    "    header = Header()\n",
    "    header.frame_id = \"base\"\n",
    "    fields = [\n",
    "        PointField('x', 0, PointField.FLOAT32, 1),\n",
    "        PointField('y', 4, PointField.FLOAT32, 1),\n",
    "        PointField('z', 8, PointField.FLOAT32, 1),\n",
    "        # PointField('rgb', 12, PointField.UINT32, 1),\n",
    "        PointField('rgba', 12, PointField.UINT32, 1),\n",
    "    ]\n",
    "    \n",
    "    cloud_pred = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], 0,0,0) for p in pred_points])\n",
    "    cloud_raw = point_cloud2.create_cloud(header, fields,\n",
    "                                [buildPoint(*p[:3], 0,0,0) for p in raw_points])\n",
    "\n",
    "    raw_pc_pub.publish(cloud_raw)\n",
    "    pred_pc_pub.publish(cloud_pred)\n",
    "    \n",
    "def image_cb(topic, msg, t, tf_buffer, v):\n",
    "    if(not len(v[\"pcbuffer\"])): return\n",
    "\n",
    "    img = rgb_msg_to_image(msg, raycastCamera.camera.is_debayered, raycastCamera.camera.rb_swap, (\"compressed\" in topic))\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "\n",
    "    if not (tf_buffer.can_transform_core(TF_MAP, TF_BASE,  msg.header.stamp)[0]): return \n",
    "    tf = tf_buffer.lookup_transform_core(TF_MAP, TF_BASE, msg.header.stamp)\n",
    "    pose = msg_to_pose(tf)  # pose in fixed ref frame (odom or map)\n",
    "\n",
    "    pc = np.concatenate(v[\"pcbuffer\"],axis = 0)\n",
    "    pred_and_checkerr(img, pc, pose, v)\n",
    "#     v[\"pcbuffer\"] = []\n",
    "    v[\"pcbuffer\"] = v[\"pcbuffer\"][-1:]\n",
    "\n",
    "player.register_callback(image_topic, image_cb)\n",
    "\n",
    "\n",
    "def pointcloud_cb(topic, msg, t, tf_buffer, v):\n",
    "    if not (tf_buffer.can_transform_core(TF_MAP, msg.header.frame_id,  msg.header.stamp)[0]): return\n",
    "    tf = tf_buffer.lookup_transform_core(TF_MAP, msg.header.frame_id,  msg.header.stamp)\n",
    "    pose = msg_to_pose(tf)\n",
    "    pc_array = rospcmsg_to_pcarray(msg, pose)\n",
    "#     v[\"pcbuffer\"].append(pc_array[:,:3])\n",
    "#     v[\"pcbuffer\"] = v[\"pcbuffer\"][-1:]\n",
    "    v[\"pcbuffer\"] = [pc_array[:,:3]]\n",
    "player.register_callback(pc_topic, pointcloud_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "duration = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a1551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"{start} -> {start+duration}\")\n",
    "player.play(start_time= player.bag.get_start_time()+start,\n",
    "            end_time = player.bag.get_start_time()+start+duration)\n",
    "start+=duration+0.05\n",
    "print(player._shared_var[\"height_range\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9033a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start+= 30\n",
    "duration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute this block to assign these two caches. \n",
    "## And then the map broadcaster in replay will only broadcast these caches\n",
    "elevmsg_cache_pred = player._shared_var[\"elevmsg_buffer\"][0]\n",
    "elevmsg_cache_raw = player._shared_var[\"elevmsg_buffer\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## Execute this block to compute the error against ground truth height\n",
    "v = player._shared_var\n",
    "((elevmap_pred, smotmap_pred), (elevmap_pc, smotmap_pc)) = v[\"elevmap_buffer\"]\n",
    "pose = v[\"pose_buffer\"]\n",
    "rz = Rotation.from_quat(pose[3:]).as_euler('xyz',degrees=False)[2]\n",
    "error_pred = evaluator.compute_error_against_gpmap(elevmap_pred, pose[:2], rz)[::-1,::-1]\n",
    "error_smooth = evaluator.compute_error_against_gpmap(smotmap_pc, pose[:2], rz)[::-1,::-1]\n",
    "error_pc = evaluator.compute_error_against_gpmap(elevmap_pc, pose[:2], rz)[::-1,::-1]\n",
    "print(error_pred[~np.isnan(error_pred)].max())\n",
    "\n",
    "vmax = 0.5\n",
    "outname = \"/home/chenyu/Pictures/new_elevmap_capture/02_err_453_\"\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(error_pred,cmap = \"RdYlBu\",vmin=0, vmax=vmax)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.savefig(outname + \"pred.png\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(error_pc,cmap = \"RdYlBu\",vmin=0, vmax=vmax)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.savefig(outname + \"pc.png\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(error_smooth,cmap = \"RdYlBu\",vmin=0, vmax=vmax)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=24)\n",
    "plt.savefig(outname + \"smooth.png\", bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d080a",
   "metadata": {},
   "source": [
    "# TMP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Image class from PIL module\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "source_dir = \"/home/chenyu/Pictures/new_trajmap_capture/\"\n",
    "target_dir = \"/home/chenyu/Pictures/new_trajmap_crop/\"\n",
    "\n",
    "# Opens a image in RGB mode\n",
    "\n",
    "for f in glob(os.path.join(source_dir, \"*.png\")):\n",
    "    if(\"err\" in f): continue\n",
    "    im = Image.open(f)\n",
    "    # Cropped image of above dimension\n",
    "    # (It will not change original image)\n",
    "    imc = im.crop((18,180+32,1761,1964))\n",
    "    imc.save(os.path.join(target_dir, os.path.basename(f)))\n",
    "\n",
    "\n",
    "for f in glob(os.path.join(target_dir, \"*.png\")):\n",
    "    im = Image.open(f)\n",
    "    # Cropped image of above dimension\n",
    "    # (It will not change original image)\n",
    "    im1 = im.crop((607,3,1138,399))\n",
    "    im2 = im.crop((0,411,1743,1752))\n",
    "    f1 = f[:-4] + \"_rgb\"+f[-4:]\n",
    "    f2 = f[:-4] + \"_viz\"+f[-4:]\n",
    "    if (\"pred\" in f1): im1.save(f1)\n",
    "    im2.save(f2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open(\"/home/chenyu/Pictures/elevmap_capture/01_pred.png\")\n",
    "import os\n",
    "# im.show()\n",
    "# display(im.crop((18,180+32,1761,1964)))\n",
    "f = \"/home/chenyu/Pictures/elevmap_capture/01_pred.png\"\n",
    "os.path.basename(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc4492",
   "metadata": {},
   "source": [
    "## Headache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "x = np.random.random(size=(10,10))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "grid1 = ImageGrid(fig, 311,\n",
    "                nrows_ncols = (1,3),\n",
    "                axes_pad = 0.05,\n",
    "                cbar_location = \"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                cbar_size=\"5%\",\n",
    "                cbar_pad=0.05\n",
    "                )\n",
    "\n",
    "grid1[0].imshow(x)\n",
    "grid1[0].axis('off')\n",
    "grid1[0].set_title('dog')\n",
    "\n",
    "grid1[1].imshow(x, cmap='hot', interpolation='nearest')\n",
    "grid1[1].axis('off')\n",
    "grid1[1].set_title('dog')\n",
    "\n",
    "imc = grid1[2].imshow(x, cmap='hot', interpolation='nearest')\n",
    "grid1[2].axis('off')\n",
    "grid1[2].set_title('dog')\n",
    "plt.colorbar(imc, cax=grid1.cbar_axes[0])\n",
    "\n",
    "\n",
    "x = x+10\n",
    "grid2 = ImageGrid(fig, 312,\n",
    "                nrows_ncols = (1,3),\n",
    "                axes_pad = 0.05,\n",
    "                cbar_location = \"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                cbar_size=\"5%\",\n",
    "                cbar_pad=0.05\n",
    "                )\n",
    "\n",
    "grid2[0].imshow(x)\n",
    "grid2[0].axis('off')\n",
    "grid2[0].set_title('dog')\n",
    "\n",
    "grid2[1].imshow(x, cmap='hot', interpolation='nearest')\n",
    "grid2[1].axis('off')\n",
    "grid2[1].set_title('dog')\n",
    "\n",
    "imc = grid2[2].imshow(x, cmap='hot', interpolation='nearest')\n",
    "grid2[2].axis('off')\n",
    "grid2[2].set_title('dog')\n",
    "plt.colorbar(imc, cax=grid2.cbar_axes[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "from matplotlib import cm\n",
    "import cv2\n",
    "\n",
    "prefix02 = \"/home/chenyu/Pictures/new_trajmap_crop/03_{}_470\"\n",
    "rgb02 = cv2.imread(prefix02.format(\"pred\") + \"_rgb.png\")[...,::-1]\n",
    "\n",
    "pred02 = cv2.imread(prefix02.format(\"pred\") + \"_viz.png\")[...,::-1]\n",
    "raw02 = cv2.imread(prefix02.format(\"raw\") + \"_viz.png\")[...,::-1]\n",
    "\n",
    "scale_percent =  raw02.shape[0] / rgb02.shape[0]\n",
    "dim = (int(rgb02.shape[1] * scale_percent), int(rgb02.shape[0] * scale_percent))\n",
    "rgb_resize02 = cv2.resize(rgb02, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "prefix06 = \"/home/chenyu/Pictures/new_trajmap_crop/06_{}_33\"\n",
    "rgb06 = cv2.imread(prefix06.format(\"pred\") + \"_rgb.png\")[...,::-1]\n",
    "\n",
    "pred06 = cv2.imread(prefix06.format(\"pred\") + \"_viz.png\")[...,::-1]\n",
    "raw06 = cv2.imread(prefix06.format(\"raw\") + \"_viz.png\")[...,::-1]\n",
    "\n",
    "scale_percent =  raw06.shape[0] / rgb06.shape[0]\n",
    "dim = (int(rgb06.shape[1] * scale_percent), int(rgb06.shape[0] * scale_percent))\n",
    "rgb_resize06 = cv2.resize(rgb06, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(50,42))\n",
    "grid = ImageGrid(fig, 111,\n",
    "                nrows_ncols = (2,3),\n",
    "                axes_pad = 0.05,\n",
    "                cbar_location = \"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                cbar_size=\"2%\",\n",
    "                cbar_pad=0.05\n",
    "                )\n",
    "\n",
    "grid[0].imshow(rgb_resize02)\n",
    "grid[0].axis('off')\n",
    "\n",
    "grid[1].imshow(pred02)\n",
    "grid[1].axis('off')\n",
    "\n",
    "grid[2].imshow(raw02)\n",
    "grid[2].axis('off')\n",
    "\n",
    "grid[3].imshow(rgb_resize06)\n",
    "grid[3].axis('off')\n",
    "\n",
    "grid[4].imshow(pred06)\n",
    "grid[4].axis('off')\n",
    "\n",
    "grid[5].imshow(raw06)\n",
    "grid[5].axis('off')\n",
    "\n",
    "cbar = plt.colorbar(cm.ScalarMappable(cmap=\"RdYlBu\"), cax=grid.cbar_axes[0])\n",
    "cbar.ax.set_yticklabels(['0','','','','','1'], fontsize=50)\n",
    "cbar.set_label('Traversability', rotation=270, fontsize=50 ,**csfont)\n",
    "\n",
    "plt.savefig(\"/home/chenyu/Pictures/traversability_compare.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41437705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
